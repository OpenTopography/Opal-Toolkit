# the base URL for the tomcat installation 
# this is required since Java can't figure out the IP 
# address if there are multiple network interfaces
tomcat.url=http://localhost:8080

# directory used for your appConfig files in order to be deployed
# if you use a relative path it is relative to your catalina.home
opal.deploy.path=deploy

# parallel parameters
num.procs=256
mpi.run=mpirun_rsh

# Slurm scheduler for submitting the job for expanse
slurm.num.procs=<number of cpu>
slurm.run=<MPI launcher command>
slurm.job.scriptdir=<the script directory for your scripts to be run in a cluster>
slurm.job.account=<the account for XSEDE allocation>
slurm.job.wdir=<working directory in a cluster>
slurm.job.queue=<the queue type in a cluster>
slurm.job.ppn=<the number of cores per node>
slurm.job.walltime=<the execution time>
slurm.job.memory_size=<memory size option>

#ssh connection to XSEDE Expanse
slurm.host=<the front end in a cluster>
slurm.port=<ssh port number>
slurm.user=<community account or user account>
slurm.password=<password>
slurm.key.file=<ssh credentials for a user>

# gateway attribute submission
gateway.attribute=true
gateway.attribute.api.key.file=/home/opentopo/.xsede-gateway-attributes-apikey
gateway.attribute.url=https://xsede-xdcdb-api.xsede.org/gateway/v2/job_attributes
gateway.attribute.xsede.resource.name=expanse.sdsc.xsede.org
gateway.attribute.user=OpenTopography User
gateway.attribute.sw.appTauDEMPitRemoveService=TauDEM:PitRemove v5.3
gateway.attribute.sw.appTauDEMDinfflowdirService=TauDEM:DinfFlowDir v5.3
gateway.attribute.sw.appTauDEMD8flowdirService=TauDEM:D8FlowDir v5.3
gateway.attribute.sw.appTauDEMAread8Service=TauDEM:AreaD8 v5.3
gateway.attribute.sw.appTauDEMAreadinfService=TauDEM:AreaDinf v5.3
gateway.attribute.sw.appTauDEMTWIService=TauDEM:TWI v5.3

# zip up input/output files, if set to true
# data.archive=true

# location of working directory relative to $CATALINA_HOME/webapps.
# this could be a symbolic link to another location (which should be
# NFS mounted if this is on a cluster). if this is a symlink, copy
# etc/opal.xml to $CATALINA_HOME/conf/Catalina/localhost/opal.xml. if
# the name of the symlink is changed to something other than "opal-jobs", 
# modify the opal.xml accordingly
# working.dir=cybergiswd

# use this key to display how long to save user data on server
opal.datalifetime=21 days

# specify in seconds the hard limit for how long a job can run
# only applicable if either DRMAA or Globus is being used, and if
# the scheduler supports it (some old version of SGE ignore the 
# parameter)
# Please be aware that your application will be killed by the scheduler 
# once it reaches the specified limit (in same cases without any log)
opal.hard_limit=36000

# full qualified class name (FQCN) of the job manager being used
opal.jobmanager=edu.sdsc.nbcr.opal.manager.ForkJobManager
# opal.jobmanager=edu.sdsc.nbcr.opal.manager.DRMAAJobManager
# opal.jobmanager=edu.sdsc.nbcr.opal.manager.GlobusJobManager
# opal.jobmanager=edu.sdsc.nbcr.opal.manager.CSFJobManager
# opal.jobmanager=edu.sdsc.nbcr.opal.manager.CondorJobManager
# opal.jobmanager=edu.sdsc.nbcr.opal.manager.RemoteGlobusJobManager

## BEGIN: information for the DRMAA job manager
## ------------------------------------------------
# the parallel environment (PE) being used by DRMAA
drmaa.pe=mpich
## ------------------------------------------------
## END: information for the DRMAA job manager

## BEGIN: information for the Globus job managers
## ------------------------------------------------ 
# url for the globus gatekeeper
globus.gatekeeper=host:2119/jobmanager-sge

# gsi information 
globus.service_cert=/path/to/your/globus/cert
globus.service_privkey=/path/to/your/globus/key

# base url for gridftp server, used by remote Globus job manager
# make sure that this directory actually exists on remote server
# use "//" to indicate absolute path, else it is interpreted as relative
# to home directory
globus.gridftp_base=gsiftp://host:2811/working_dir
## ------------------------------------------------
## END: information for the Globus job manager

## BEGIN: information for the CSF job manager
## ------------------------------------------------
# CSF working directory
# This is the remote directory under the remote user's home directory
csf4.workingDir=opal_runs
## ------------------------------------------------
## END: information for the CSF job manager

## BEGIN: information for the Condor job manager
## ------------------------------------------------
# the script used by Condor to launch MPI jobs
mpi.script=/opt/condor/etc/examples/mp1script
## ------------------------------------------------
## END: information for the Condor job manager

## BEGIN: information for the per IP limits on job submission
## ----------------------------------------------------------
# boolean switch to turn processing on or off
opal.ip.processing=true

# number of jobs per IP per hour
opal.ip.limit=500

# block all jobs from this IP - comma separated entries (optional)
opal.ip.blacklist=66.102.7.104

# always allow jobs from this IP - comma separated entries (optional)
# opal.ip.whitelist=127.0.0.1,198.202.90.122,198.202.90.135,198.202.90.254,216.171.17.214,132.249.20.244,99.10.121.114
opal.ip.whitelist=127.0.0.1
## ----------------------------------------------------------
## END: information for the per IP limits on job submission

## BEGIN: information for Fork job manager
## ----------------------------------------------------------
# number of jobs that can be in execution simultaneously
fork.jobs.limit=4
## ----------------------------------------------------------
## END: information for For job manager
